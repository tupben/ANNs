Here are two neural nets I built, based on [this](http://iamtrask.github.io/2015/07/12/basic-python-network/) tutorial.


backprop.py is the most basic neural network. It learns to return the value of the first item of the set.


simple_neuron.py has 2 hidden layers. It can learn basic logic-gate functions.


Both networks are set up to use either ReLu or Sigmoid activation functions. I know that ReLu is "in" and sigmoid is "out", but these work much better with sigmoid functions. Maybe I'm using ReLu wrong??


Your math enthusiast,

Ben Tupper